@article{Peyron2021LAwithAE,
	author   = {Peyron, Mathis and Fillion, Anthony and Gürol, Selime and Marchais, Victor and Gratton, Serge and Boudier, Pierre and Goret, Gael},
	title    = {Latent space data assimilation by using deep learning},
	journal  = {Quarterly Journal of the Royal Meteorological Society},
	volume   = {147},
	number   = {740},
	pages    = {3759-3777},
	keywords = {autoencoders, data assimilation, deep learning, latent space, Lorenz 96, surrogate model},
	doi      = {https://doi.org/10.1002/qj.4153},
	url      = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.4153},
	eprint   = {https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.4153},
	year     = {2021}
}
@inproceedings{bachlechner2021rezero,
	title        = {Rezero is all you need: Fast convergence at large depth},
	author       = {Bachlechner, Thomas and Majumder, Bodhisattwa Prasad and Mao, Henry and Cottrell, Gary and McAuley, Julian},
	booktitle    = {Uncertainty in Artificial Intelligence},
	pages        = {1352--1361},
	year         = {2021},
	organization = {PMLR}
}
@article{Fillion2020IEnKS,
	author  = {Fillion, Anthony and Bocquet, Marc and Gratton, Serge and G\"{u}rol, Selime and Sakov, Pavel},
	title   = {An Iterative Ensemble Kalman Smoother in Presence of Additive Model Error},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	volume  = {8},
	number  = {1},
	pages   = {198-228},
	year    = {2020},
	doi     = {10.1137/19M1244147},
	url     = {https://doi.org/10.1137/19M1244147},
	eprint  = {https://doi.org/10.1137/19M1244147}
}
@article{Galewsky-2004,
	abstract = {We present an initial-value problem for testing numerical models of the global shallow-water equations. This new test case is designed to address some of the difficulties that have recently been uncovered in the canonical test case suite of Williamson et al. The new test case is simple to set up, yet able to generate a complex and realistic flow. The initial condition consists of an analytically specified, balanced, barotropically unstable, mid-latitude jet, to which a simple perturbation is added to initiate the instability. The evolution is comprised of an early adjustment phase dominated by fast, gravity wave dynamics, and a later development characterized by the slow, nearly balanced roll-up of the vorticity field associated with the initial jet.We compute solutions to this problem with a spectral transform model to numerical convergence, in the sense that we refine the spatial and temporal resolution until no changes can be visually detected in global contour plots of the solution fields. We also quantify the convergence with standard norms. We validate these solutions by recomputing them with a different model, and show that the solutions thus obtained converge to those of the original model. This new test is intended to serve as a complement to the Williamson et al. suite, and should be of particular interest in that it involves the formation of complicated dynamical features similar to those that arise in numerical weather prediction and climate models.},
	author   = {Galewsky, Joseph and Scott, Richard K. and Polvani, Lorenzo M.},
	doi      = {10.3402/tellusa.v56i5.14436},
	journal  = {Tellus A: Dynamic Meteorology and Oceanography},
	keyword  = {},
	month    = {Jan},
	title    = {An initial-value problem for testing numerical models of the global shallow-water equations},
	year     = {2004}
}
@article{AEflow,
	title     = {Deep learning for in situ data compression of large turbulent flow simulations},
	author    = {Glaws, Andrew and King, Ryan and Sprague, Michael},
	journal   = {Phys. Rev. Fluids},
	volume    = {5},
	issue     = {11},
	pages     = {114602},
	numpages  = {23},
	year      = {2020},
	month     = {Nov},
	publisher = {American Physical Society},
	doi       = {10.1103/PhysRevFluids.5.114602},
	url       = {https://link.aps.org/doi/10.1103/PhysRevFluids.5.114602}
}
@article{yin2023dino,
	title   = {Continuous PDE Dynamics Forecasting with Implicit Neural Representations},
	author  = {Yin, Yuan and Kirchmeyer, Matthieu and Franceschi, Jean-Yves and Rakotomamonjy, Alain and Gallinari, Patrick},
	journal = {arXiv preprint arXiv:2209.14855},
	year    = {2022}
}
@inproceedings{Park2019Auto-decoding,
	author    = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
	booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	title     = {DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation},
	year      = {2019},
	volume    = {},
	number    = {},
	pages     = {165-174},
	doi       = {10.1109/CVPR.2019.00025}
}
@article{kim2019attentive,
	title   = {Attentive neural processes},
	author  = {Kim, Hyunjik and Mnih, Andriy and Schwarz, Jonathan and Garnelo, Marta and Eslami, Ali and Rosenbaum, Dan and Vinyals, Oriol and Teh, Yee Whye},
	journal = {arXiv preprint arXiv:1901.05761},
	year    = {2019}
}
@inproceedings{fathony2021multiplicative,
	title     = {Multiplicative filter networks},
	author    = {Fathony, Rizal and Sahu, Anit Kumar and Willmott, Devin and Kolter, J Zico},
	booktitle = {International Conference on Learning Representations},
	year      = {2021}
}
@article{chen2018NeuralODE,
	title   = {Neural ordinary differential equations},
	author  = {Chen, Ricky TQ and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
	journal = {Advances in neural information processing systems},
	volume  = {31},
	year    = {2018}
}
@article{chen2021eventfn,
	title   = {Learning Neural Event Functions for Ordinary Differential Equations},
	author  = {Chen, Ricky T. Q. and Amos, Brandon and Nickel, Maximilian},
	journal = {International Conference on Learning Representations},
	year    = {2021}
}

@inbook{Goan2020BNNsurvey,
	author    = {Goan, Ethan
	             and Fookes, Clinton},
	editor    = {Mengersen, Kerrie L.
	             and Pudlo, Pierre
	             and Robert, Christian P.},
	title     = {Bayesian Neural Networks: An Introduction and Survey},
	booktitle = {Case Studies in Applied Bayesian Data Science: CIRM Jean-Morlet Chair, Fall 2018},
	year      = {2020},
	publisher = {Springer International Publishing},
	address   = {Cham},
	pages     = {45--87},
	isbn      = {978-3-030-42553-1},
	doi       = {10.1007/978-3-030-42553-1_3},
	url       = {https://doi.org/10.1007/978-3-030-42553-1_3}
}
@inproceedings{Chattopadhyay2021DeepST,
	author    = {Chattopadhyay, Ashesh and Mustafa, Mustafa and Hassanzadeh, Pedram and Kashinath, Karthik},
	title     = {Deep Spatial Transformers for Autoregressive Data-Driven Forecasting of Geophysical Turbulence},
	year      = {2021},
	isbn      = {9781450388481},
	publisher = {Association for Computing Machinery},
	address   = {New York, NY, USA},
	url       = {https://doi.org/10.1145/3429309.3429325},
	doi       = {10.1145/3429309.3429325},
	booktitle = {Proceedings of the 10th International Conference on Climate Informatics},
	pages     = {106-112},
	numpages  = {7},
	keywords  = {spatio-temporal, spatial transformers, geophysical turbulence, equivariance},
	location  = {virtual, United Kingdom},
	series    = {CI2020}
}
@article{Schultz2021CanDL,
	author  = {Schultz, M. G.  and Betancourt, C.  and Gong, B.  and Kleinert, F.  and Langguth, M.  and Leufen, L. H.  and Mozaffari, A.  and Stadtler, S. },
	title   = {Can deep learning beat numerical weather prediction?},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	volume  = {379},
	number  = {2194},
	pages   = {20200097},
	year    = {2021},
	doi     = {10.1098/rsta.2020.0097},
	url     = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2020.0097}
}
@article{LatentspaceDA-RNN,
	author   = {Penny, S. G. and Smith, T. A. and Chen, T.-C. and Platt, J. A. and Lin, H.-Y. and Goodliff, M. and Abarbanel, H. D. I.},
	title    = {Integrating Recurrent Neural Networks With Data Assimilation for Scalable Data-Driven State Estimation},
	journal  = {Journal of Advances in Modeling Earth Systems},
	volume   = {14},
	number   = {3},
	pages    = {e2021MS002843},
	keywords = {data assimilation, recurrent neural networks, machine learning, artificial intelligence, ensemble kalman filter, 4D-var},
	doi      = {10.1029/2021MS002843},
	year     = {2022}
}
@article{ReservoirComputing,
	title   = {Reservoir computing approaches to recurrent neural network training},
	journal = {Computer Science Review},
	volume  = {3},
	number  = {3},
	pages   = {127-149},
	year    = {2009},
	issn    = {1574-0137},
	doi     = {10.1016/j.cosrev.2009.03.005},
	url     = {https://www.sciencedirect.com/science/article/pii/S1574013709000173},
	author  = {Mantas Lukoševičius and Herbert Jaeger}
}
@article{Arcomano2020MLGAFM,
	author  = {Arcomano, Troy and Szunyogh, Istvan and Pathak, Jaideep and Wikner, Alexander and Hunt, Brian R. and Ott, Edward},
	title   = {A Machine Learning-Based Global Atmospheric Forecast Model},
	journal = {Geophysical Research Letters},
	volume  = {47},
	number  = {9},
	pages   = {e2020GL087776},
	doi     = {https://doi.org/10.1029/2020GL087776},
	url     = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020GL087776},
	year    = {2020}
}
@article{ROM-DA,
	author   = {Pawar, Suraj and San, Omer},
	title    = {Equation-Free Surrogate Modeling of Geophysical Flows at the Intersection of Machine Learning and Data Assimilation},
	journal  = {Journal of Advances in Modeling Earth Systems},
	volume   = {14},
	number   = {11},
	pages    = {e2022MS003170},
	keywords = {reduced order modeling, data assimilation, deep learning, optimal sensor placement, nonintrusive modeling, see surface temperature},
	doi      = {10.1029/2022MS003170},
	url      = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2022MS003170},
	year     = {2022}
}
@article{GeneralizedLA,
	author     = {Cheng, Sibo and Chen, Jianhua and Anastasiou, Charitos and Angeli, Panagiota and Matar, Omar K. and Guo, Yi-Ke and Pain, Christopher C. and Arcucci, Rossella},
	title      = {Generalised Latent Assimilation in Heterogeneous Reduced Spaces with Machine Learning Surrogate Models},
	year       = {2023},
	issue_date = {Jan 2023},
	publisher  = {Plenum Press},
	address    = {USA},
	volume     = {94},
	number     = {1},
	issn       = {0885-7474},
	url        = {https://doi.org/10.1007/s10915-022-02059-4},
	doi        = {10.1007/s10915-022-02059-4},
	journal    = {J. Sci. Comput.},
	month      = {jan},
	numpages   = {37},
	keywords   = {Data assimilation, Reduced-order-modelling, Recurrent neural networks, Deep learning, Explainable AI}
}
@article{dedalus,
	title     = {Dedalus: A flexible framework for numerical simulations with spectral methods},
	author    = {Burns, Keaton J. and Vasil, Geoffrey M. and Oishi, Jeffrey S. and Lecoanet, Daniel and Brown, Benjamin P.},
	journal   = {Phys. Rev. Res.},
	volume    = {2},
	issue     = {2},
	pages     = {023068},
	numpages  = {39},
	year      = {2020},
	month     = {Apr},
	publisher = {American Physical Society},
	doi       = {10.1103/PhysRevResearch.2.023068},
	url       = {https://link.aps.org/doi/10.1103/PhysRevResearch.2.023068}
}
@article{Nabizadeh2019QGcode,
	author   = {Nabizadeh, Ebrahim and Hassanzadeh, Pedram and Yang, Da and Barnes, Elizabeth A.},
	title    = {Size of the Atmospheric Blocking Events: Scaling Law and Response to Climate Change},
	journal  = {Geophysical Research Letters},
	volume   = {46},
	number   = {22},
	pages    = {13488-13499},
	keywords = {blocking events, extreme events, climate change, scaling law, midlatitude circulation, hierarchical modeling},
	doi      = {https://doi.org/10.1029/2019GL084863},
	url      = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019GL084863},
	year     = {2019}
}
@article{Nicholas2015QGsetting,
	author    = {Nicholas J. Lutsko and Isaac M. Held and Pablo Zurita-Gotor},
	title     = {Applying the Fluctuation-Dissipation Theorem to a Two-Layer Model of Quasigeostrophic Turbulence},
	journal   = {Journal of the Atmospheric Sciences},
	year      = {2015},
	publisher = {American Meteorological Society},
	address   = {Boston MA, USA},
	volume    = {72},
	number    = {8},
	doi       = {https://doi.org/10.1175/JAS-D-14-0356.1},
	pages     = {3161 - 3177},
	url       = {https://journals.ametsoc.org/view/journals/atsc/72/8/jas-d-14-0356.1.xml}
}
@inproceedings{chen2019learning,
	title     = {Learning implicit fields for generative shape modeling},
	author    = {Chen, Zhiqin and Zhang, Hao},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages     = {5939--5948},
	year      = {2019}
}
@inproceedings{park2019deepsdf,
	title     = {Deepsdf: Learning continuous signed distance functions for shape representation},
	author    = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
	booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages     = {165--174},
	year      = {2019}
}
@inproceedings{Jiang2020MeshfreeFlowNet,
	author    = {Jiang, Chiyu "Max" and Esmaeilzadeh, Soheil and Azizzadenesheli, Kamyar and Kashinath, Karthik and Mustafa, Mustafa and Tchelepi, Hamdi A. and Marcus, Philip and Prabhat and Anandkumar, Anima},
	title     = {MeshfreeFlowNet: A Physics-Constrained Deep Continuous Space-Time Super-Resolution Framework},
	year      = {2020},
	isbn      = {9781728199986},
	publisher = {IEEE Press},
	booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
	articleno = {9},
	numpages  = {15},
	keywords  = {PDEs, physics-constrained, deep neural networks, super-resolution},
	location  = {Atlanta, Georgia},
	series    = {SC '20}
}
@inproceedings{sitzmann2019siren,
	author    = {Sitzmann, Vincent
	             and Martel, Julien N.P.
	             and Bergman, Alexander W.
	             and Lindell, David B.
	             and Wetzstein, Gordon},
	title     = {Implicit Neural Representations
	             with Periodic Activation Functions},
	booktitle = {arXiv},
	year      = {2020}
}
@article{Bemana2020xfields,
	author  = {Bemana, Mojtaba and Myszkowski, Karol and Seidel, Hans-Peter and Ritschel, Tobias},
	title   = {X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation},
	journal = {ACM Transactions on Graphics (Proc. SIGGRAPH Asia 2020)},
	year    = {2020},
	volume  = {39},
	number  = {6},
	doi     = {10.1145/3414685.3417827}
}
@article{dupont2022coinpp,
	title   = {Coin++: Data agnostic neural compression},
	author  = {Dupont, Emilien and Loya, Hrushikesh and Alizadeh, Milad and Goli{\'n}ski, Adam and Teh, Yee Whye and Doucet, Arnaud},
	journal = {arXiv preprint arXiv:2201.12904},
	year    = {2022}
}
@inproceedings{chen2023crom,
	title     = {{CROM}: Continuous Reduced-Order Modeling of {PDE}s Using Implicit Neural Representations},
	author    = {Peter Yichen Chen and Jinxu Xiang and Dong Heon Cho and Yue Chang and G A Pershing and Henrique Teles Maia and Maurizio M Chiaramonte and Kevin Thomas Carlberg and Eitan Grinspun},
	booktitle = {The Eleventh International Conference on Learning Representations },
	year      = {2023},
	url       = {https://openreview.net/forum?id=FUORz1tG8Og}
}

@inproceedings{dupont2022,
	title     = {From data to functa: Your data point is a function and you can treat it like one},
	author    = {Dupont, Emilien and Kim, Hyunjik and Eslami, S. M. Ali and Rezende, Danilo Jimenez and Rosenbaum, Dan},
	booktitle = {Proceedings of the 39th International Conference on Machine Learning},
	pages     = {5694--5725},
	year      = {2022},
	editor    = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume    = {162},
	series    = {Proceedings of Machine Learning Research},
	month     = {17--23 Jul},
	publisher = {PMLR},
	pdf       = {https://proceedings.mlr.press/v162/dupont22a/dupont22a.pdf},
	url       = {https://proceedings.mlr.press/v162/dupont22a.html}
}
@article{Weller2012SkipLatLon,
	author    = {Hilary Weller and John Thuburn and Colin J. Cotter},
	title     = {Computational Modes and Grid Imprinting on Five Quasi-Uniform Spherical C Grids},
	journal   = {Monthly Weather Review},
	year      = {2012},
	publisher = {American Meteorological Society},
	address   = {Boston MA, USA},
	volume    = {140},
	number    = {8},
	doi       = {https://doi.org/10.1175/MWR-D-11-00193.1},
	pages     = {2734 - 2755},
	url       = {https://journals.ametsoc.org/view/journals/mwre/140/8/mwr-d-11-00193.1.xml}
}
@article{PINNs,
	title     = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	author    = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
	journal   = {Journal of Computational Physics},
	volume    = {378},
	pages     = {686--707},
	year      = {2019},
	publisher = {Elsevier}
}
@article{DeepRitz,
	title     = {The Deep Ritz Method: A Deep Learning-Based Numerical Algorithm for Solving Variational Problems},
	keywords  = {Deep Ritz Method, Eigenvalue problems, PDE, Variational problems},
	author    = {E. Weinan and Bing Yu},
	year      = {2018},
	month     = mar,
	day       = {1},
	doi       = {10.1007/s40304-018-0127-z},
	language  = {English (US)},
	volume    = {6},
	pages     = {1--12},
	journal   = {Communications in Mathematics and Statistics},
	issn      = {2194-6701},
	publisher = {Springer Verlag},
	number    = {1}
}
@article{DeepGalerkin,
	title    = {DGM: A deep learning algorithm for solving partial differential equations},
	journal  = {Journal of Computational Physics},
	volume   = {375},
	pages    = {1339-1364},
	year     = {2018},
	issn     = {0021-9991},
	doi      = {https://doi.org/10.1016/j.jcp.2018.08.029},
	url      = {https://www.sciencedirect.com/science/article/pii/S0021999118305527},
	author   = {Justin Sirignano and Konstantinos Spiliopoulos},
	keywords = {Partial differential equations, Machine learning, Deep learning, High-dimensional partial differential equations},
	abstract = {High-dimensional PDEs have been a longstanding computational challenge. We propose to solve high-dimensional PDEs by approximating the solution with a deep neural network which is trained to satisfy the differential operator, initial condition, and boundary conditions. Our algorithm is meshfree, which is key since meshes become infeasible in higher dimensions. Instead of forming a mesh, the neural network is trained on batches of randomly sampled time and space points. The algorithm is tested on a class of high-dimensional free boundary PDEs, which we are able to accurately solve in up to 200 dimensions. The algorithm is also tested on a high-dimensional Hamilton–Jacobi–Bellman PDE and Burgers' equation. The deep learning algorithm approximates the general solution to the Burgers' equation for a continuum of different boundary conditions and physical conditions (which can be viewed as a high-dimensional space). We call the algorithm a “Deep Galerkin Method (DGM)” since it is similar in spirit to Galerkin methods, with the solution approximated by a neural network instead of a linear combination of basis functions. In addition, we prove a theorem regarding the approximation power of neural networks for a class of quasilinear parabolic PDEs.}
}
@article{PDENet2.0,
	title    = {PDE-Net 2.0: Learning PDEs from data with a numeric-symbolic hybrid deep network},
	journal  = {Journal of Computational Physics},
	volume   = {399},
	pages    = {108925},
	year     = {2019},
	issn     = {0021-9991},
	doi      = {https://doi.org/10.1016/j.jcp.2019.108925},
	url      = {https://www.sciencedirect.com/science/article/pii/S0021999119306308},
	author   = {Zichao Long and Yiping Lu and Bin Dong},
	keywords = {Partial differential equations, Dynamic system, Convolutional neural network, Symbolic neural network},
	abstract = {Partial differential equations (PDEs) are commonly derived based on empirical observations. However, recent advances of technology enable us to collect and store massive amount of data, which offers new opportunities for data-driven discovery of PDEs. In this paper, we propose a new deep neural network, called PDE-Net 2.0, to discover (time-dependent) PDEs from observed dynamic data with minor prior knowledge on the underlying mechanism that drives the dynamics. The design of PDE-Net 2.0 is based on our earlier work [1] where the original version of PDE-Net was proposed. PDE-Net 2.0 is a combination of numerical approximation of differential operators by convolutions and a symbolic multi-layer neural network for model recovery. Comparing with existing approaches, PDE-Net 2.0 has the most flexibility and expressive power by learning both differential operators and the nonlinear response function of the underlying PDE model. Numerical experiments show that the PDE-Net 2.0 has the potential to uncover the hidden PDE of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment.}
}
@article{SINDy,
	author   = {Steven L. Brunton  and Joshua L. Proctor  and J. Nathan Kutz },
	title    = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
	journal  = {Proceedings of the National Academy of Sciences},
	volume   = {113},
	number   = {15},
	pages    = {3932-3937},
	year     = {2016},
	doi      = {10.1073/pnas.1517384113},
	url      = {https://www.pnas.org/doi/abs/10.1073/pnas.1517384113},
	eprint   = {https://www.pnas.org/doi/pdf/10.1073/pnas.1517384113},
	abstract = {Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.}
}
@article{AI-Feyman,
	author  = {Silviu-Marian Udrescu  and Max Tegmark },
	title   = {AI Feynman: A physics-inspired method for symbolic regression},
	journal = {Science Advances},
	volume  = {6},
	number  = {16},
	pages   = {eaay2631},
	year    = {2020},
	doi     = {10.1126/sciadv.aay2631},
	url     = {https://www.science.org/doi/abs/10.1126/sciadv.aay2631},
	eprint  = {https://www.science.org/doi/pdf/10.1126/sciadv.aay2631}
}
@article{DeepONet,
	title   = {Learning nonlinear operators via {DeepONet} based on the universal approximation theorem of operators},
	author  = {Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em},
	journal = {Nature Machine Intelligence},
	volume  = {3},
	number  = {3},
	pages   = {218--229},
	year    = {2021}
}
@misc{FNO,
	title         = {Fourier Neural Operator for Parametric Partial Differential Equations},
	author        = {Zongyi Li and Nikola Kovachki and Kamyar Azizzadenesheli and Burigede Liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
	year          = {2020},
	eprint        = {2010.08895},
	archiveprefix = {arXiv},
	primaryclass  = {cs.LG}
}
@article{LSTM,
	author     = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
	title      = {Long Short-Term Memory},
	year       = {1997},
	issue_date = {November 15, 1997},
	publisher  = {MIT Press},
	address    = {Cambridge, MA, USA},
	volume     = {9},
	number     = {8},
	issn       = {0899-7667},
	url        = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi        = {10.1162/neco.1997.9.8.1735},
	abstract   = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	journal    = {Neural Comput.},
	month      = {nov},
	pages      = {1735-1780},
	numpages   = {46}
}
@article{GRU,
	author     = {Junyoung Chung and
	              {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
	              KyungHyun Cho and
	              Yoshua Bengio},
	title      = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
	              Modeling},
	journal    = {CoRR},
	volume     = {abs/1412.3555},
	year       = {2014},
	url        = {http://arxiv.org/abs/1412.3555},
	eprinttype = {arXiv},
	eprint     = {1412.3555},
	timestamp  = {Mon, 13 Aug 2018 16:47:38 +0200},
	biburl     = {https://dblp.org/rec/journals/corr/ChungGCB14.bib},
	bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@article{ERA5,
	author   = {Hersbach, Hans and Bell, Bill and Berrisford, Paul and Hirahara, Shoji and Horányi, András and Muñoz-Sabater, Joaquín and Nicolas, Julien and Peubey, Carole and Radu, Raluca and Schepers, Dinand and Simmons, Adrian and Soci, Cornel and Abdalla, Saleh and Abellan, Xavier and Balsamo, Gianpaolo and Bechtold, Peter and Biavati, Gionata and Bidlot, Jean and Bonavita, Massimo and De Chiara, Giovanna and Dahlgren, Per and Dee, Dick and Diamantakis, Michail and Dragani, Rossana and Flemming, Johannes and Forbes, Richard and Fuentes, Manuel and Geer, Alan and Haimberger, Leo and Healy, Sean and Hogan, Robin J. and Hólm, Elías and Janisková, Marta and Keeley, Sarah and Laloyaux, Patrick and Lopez, Philippe and Lupu, Cristina and Radnoti, Gabor and de Rosnay, Patricia and Rozum, Iryna and Vamborg, Freja and Villaume, Sebastien and Thépaut, Jean-Noël},
	title    = {The ERA5 global reanalysis},
	journal  = {Quarterly Journal of the Royal Meteorological Society},
	volume   = {146},
	number   = {730},
	pages    = {1999-2049},
	keywords = {climate reanalysis, Copernicus Climate Change Service, data assimilation, ERA5, historical observations},
	doi      = {https://doi.org/10.1002/qj.3803},
	url      = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.3803},
	eprint   = {https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.3803},
	year     = {2020}
}
@book{Bocquet2016,
	author    = {Asch, Mark and Bocquet, Marc and Nodet, Maëlle},
	title     = {Data Assimilation},
	publisher = {Society for Industrial and Applied Mathematics},
	year      = {2016},
	doi       = {10.1137/1.9781611974546},
	address   = {Philadelphia, PA},
	edition   = {},
	url       = {https://epubs.siam.org/doi/abs/10.1137/1.9781611974546},
	eprint    = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611974546}
}
@article{FourCastNet,
	title   = {Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators},
	author  = {Pathak, Jaideep and Subramanian, Shashank and Harrington, Peter and Raja, Sanjeev and Chattopadhyay, Ashesh and Mardani, Morteza and Kurth, Thorsten and Hall, David and Li, Zongyi and Azizzadenesheli, Kamyar and Hassanzadeh, Pedram and Kashinath, Karthik and Anandkumar, Animashree},
	journal = {arXiv preprint arXiv:2202.11214},
	year    = {2022}
}
@article{Pangu,
	title   = {Pangu-Weather: A 3D High-Resolution Model for Fast and Accurate Global Weather Forecast},
	author  = {Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
	journal = {arXiv preprint arXiv:2211.02556},
	year    = {2022}
}
@article{GraphCast,
	title   = {GraphCast: Learning skillful medium-range global weather forecasting},
	author  = {Lam, Remi and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Pritzel, Alexander and Ravuri, Suman and Ewalds, Timo and Alet, Ferran and Eaton-Rosen, Zach and others},
	journal = {arXiv preprint arXiv:2212.12794},
	year    = {2022}
}
@article{FengWu,
	title   = {FengWu: Pushing the Skillful Global Medium-range Weather Forecast beyond 10 Days Lead},
	author  = {Chen, Kang and Han, Tao and Gong, Junchao and Bai, Lei and Ling, Fenghua and Luo, Jing-Jia and Chen, Xi and Ma, Leiming and Zhang, Tianning and Su, Rui and others},
	journal = {arXiv preprint arXiv:2304.02948},
	year    = {2023}
}
@article{He2015ResNet,
	author  = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	title   = {Deep Residual Learning for Image Recognition},
	journal = {arXiv preprint arXiv:1512.03385},
	year    = {2015}
}
@article{kingma2014adam,
	title   = {Adam: A method for stochastic optimization},
	author  = {Kingma, Diederik P and Ba, Jimmy},
	journal = {arXiv preprint arXiv:1412.6980},
	year    = {2014}
}
@article{Rasp2020WeatherBench,
	author   = {Rasp, Stephan and Dueben, Peter D. and Scher, Sebastian and Weyn, Jonathan A. and Mouatadid, Soukayna and Thuerey, Nils},
	title    = {WeatherBench: A Benchmark Data Set for Data-Driven Weather Forecasting},
	journal  = {Journal of Advances in Modeling Earth Systems},
	volume   = {12},
	number   = {11},
	pages    = {e2020MS002203},
	keywords = {machine learning, NWP, artificial intelligence, benchmark},
	doi      = {https://doi.org/10.1029/2020MS002203},
	url      = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002203},
	eprint   = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002203},
	note     = {e2020MS002203 10.1029/2020MS002203},
	year     = {2020}
}
@article{Clare2021,
	author   = {Clare, Mariana C.A. and Jamil, Omar and Morcrette, Cyril J.},
	title    = {Combining distribution-based neural networks to predict weather forecast probabilities},
	journal  = {Quarterly Journal of the Royal Meteorological Society},
	volume   = {147},
	number   = {741},
	pages    = {4337-4357},
	keywords = {data exploration, deep learning, ensemble dropout, probabilistic weather forecasting, probability density functions, ResNet, stacked neural network},
	doi      = {https://doi.org/10.1002/qj.4180},
	url      = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.4180},
	eprint   = {https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.4180},
	year     = {2021}
}
@article{Scher2021Ensemble,
	author   = {Scher, Sebastian and Messori, Gabriele},
	title    = {Ensemble Methods for Neural Network-Based Weather Forecasts},
	journal  = {Journal of Advances in Modeling Earth Systems},
	volume   = {13},
	number   = {2},
	pages    = {},
	keywords = {ensemble forecasting, machine learning, neural networks, singular value decomposition, weather forecasting},
	doi      = {https://doi.org/10.1029/2020MS002331},
	url      = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002331},
	eprint   = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002331},
	abstract = {Abstract Ensemble weather forecasts enable a measure of uncertainty to be attached to each forecast, by computing the ensemble's spread. However, generating an ensemble with a good spread-error relationship is far from trivial, and a wide range of approaches to achieve this have been explored—chiefly in the context of numerical weather prediction models. Here, we aim to transform a deterministic neural network weather forecasting system into an ensemble forecasting system. We test four methods to generate the ensemble: random initial perturbations, retraining of the neural network, use of random dropout in the network, and the creation of initial perturbations with singular vector decomposition. The latter method is widely used in numerical weather prediction models, but is yet to be tested on neural networks. The ensemble mean forecasts obtained from these four approaches all beat the unperturbed neural network forecasts, with the retraining method yielding the highest improvement. However, the skill of the neural network forecasts is systematically lower than that of state-of-the-art numerical weather prediction models.},
	year     = {2021}
}
@article{Weyn2020,
	author   = {Weyn, Jonathan A. and Durran, Dale R. and Caruana, Rich},
	title    = {Improving Data-Driven Global Weather Prediction Using Deep Convolutional Neural Networks on a Cubed Sphere},
	journal  = {Journal of Advances in Modeling Earth Systems},
	volume   = {12},
	number   = {9},
	pages    = {e2020MS002109},
	doi      = {https://doi.org/10.1029/2020MS002109},
	url      = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002109},
	eprint   = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002109},
	note     = {e2020MS002109 10.1029/2020MS002109},
	abstract = {Abstract We present a significantly improved data-driven global weather forecasting framework using a deep convolutional neural network (CNN) to forecast several basic atmospheric variables on a global grid. New developments in this framework include an off-line volume-conservative mapping to a cubed-sphere grid, improvements to the CNN architecture and the minimization of the loss function over multiple steps in a prediction sequence. The cubed-sphere remapping minimizes the distortion on the cube faces on which convolution operations are performed and provides natural boundary conditions for padding in the CNN. Our improved model produces weather forecasts that are indefinitely stable and produce realistic weather patterns at lead times of several weeks and longer. For short- to medium-range forecasting, our model significantly outperforms persistence, climatology, and a coarse-resolution dynamical numerical weather prediction (NWP) model. Unsurprisingly, our forecasts are worse than those from a high-resolution state-of-the-art operational NWP system. Our data-driven model is able to learn to forecast complex surface temperature patterns from few input atmospheric state variables. On annual time scales, our model produces a realistic seasonal cycle driven solely by the prescribed variation in top-of-atmosphere solar forcing. Although it currently does not compete with operational weather forecasting models, our data-driven CNN executes much faster than those models, suggesting that machine learning could prove to be a valuable tool for large-ensemble forecasting.},
	year     = {2020}
}
@article{Rasp2021,
	author   = {Rasp, Stephan and Thuerey, Nils},
	title    = {Data-Driven Medium-Range Weather Prediction With a Resnet Pretrained on Climate Simulations: A New Model for WeatherBench},
	journal  = {Journal of Advances in Modeling Earth Systems},
	volume   = {13},
	number   = {2},
	pages    = {e2020MS002405},
	keywords = {deep learning, machine learning, numerical weather forecasting},
	doi      = {https://doi.org/10.1029/2020MS002405},
	url      = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002405},
	eprint   = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002405},
	note     = {e2020MS002405 2020MS002405},
	year     = {2021}
}

