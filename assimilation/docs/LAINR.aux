\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Leveraging Reduced-Order-Models for Latent Assimilation}{1}{subsection.1.1}}
\citation{PINNs}
\citation{DeepRitz}
\citation{DeepGalerkin}
\citation{PDENet2.0}
\citation{SINDy}
\citation{AI-Feyman}
\citation{DeepONet}
\citation{FNO}
\citation{Schultz2021CanDL}
\citation{FourCastNet}
\citation{Pangu}
\citation{GraphCast}
\citation{FengWu}
\citation{Peyron2021LAwithAE}
\citation{LatentspaceDA-RNN}
\citation{ROM-DA}
\citation{GeneralizedLA}
\citation{Peyron2021LAwithAE}
\citation{bachlechner2021rezero}
\citation{He2015ResNet}
\citation{LatentspaceDA-RNN}
\citation{ReservoirComputing}
\citation{Arcomano2020MLGAFM}
\citation{LatentspaceDA-RNN}
\citation{ROM-DA}
\citation{GeneralizedLA}
\citation{Peyron2021LAwithAE}
\citation{Peyron2021LAwithAE}
\citation{LatentspaceDA-RNN}
\citation{ROM-DA}
\citation{GeneralizedLA}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of different methods for latent assimilation. The method names are borrowed from the original papers.\relax }}{2}{table.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:related_works}{{1}{2}{Comparison of different methods for latent assimilation. The method names are borrowed from the original papers.\relax }{table.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Related works and contributions}{2}{subsection.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Latent-space embedding and dynamics learning}{3}{section.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison between classical DA and LA frameworks.\relax }}{3}{figure.caption.2}}
\newlabel{fig:comp-DA-and-LA}{{1}{3}{Comparison between classical DA and LA frameworks.\relax }{figure.caption.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison of different methods for latent embedding.\relax }}{3}{table.caption.3}}
\newlabel{tab:comp_latent_embedding}{{2}{3}{Comparison of different methods for latent embedding.\relax }{table.caption.3}{}}
\citation{LSTM}
\citation{GRU}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Classical linear ROMs}{4}{subsection.2.1}}
\newlabel{sec:linear-encoders}{{2.1}{4}{Classical linear ROMs}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Autoencoders: non-linear representations and limitations}{4}{subsection.2.2}}
\citation{chen2019learning}
\citation{park2019deepsdf}
\citation{sitzmann2019siren}
\citation{Bemana2020xfields}
\citation{dupont2022coinpp}
\citation{Jiang2020MeshfreeFlowNet}
\citation{chen2023crom}
\citation{yin2023dino}
\citation{LSTM}
\citation{GRU}
\citation{He2015ResNet}
\citation{chen2018NeuralODE}
\citation{chen2021eventfn}
\citation{chen2018NeuralODE}
\citation{chen2021eventfn}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Implicit neural representations}{5}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Evolution of latent dynamics}{5}{subsection.2.4}}
\citation{Peyron2021LAwithAE}
\citation{LatentspaceDA-RNN}
\citation{Galewsky-2004}
\citation{Galewsky-2004}
\@writefile{toc}{\contentsline {section}{\numberline {3}Latent assimilation framework}{6}{section.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{6}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Test cases}{6}{subsection.4.1}}
\newlabel{sec:test-eqs}{{4.1}{6}{Test cases}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}the shallow-water model}{6}{subsubsection.4.1.1}}
\citation{ERA5}
\citation{Rasp2020WeatherBench}
\citation{Clare2021}
\citation{Scher2021Ensemble}
\citation{Weyn2020}
\citation{Rasp2021}
\citation{Peyron2021LAwithAE}
\citation{Peyron2021LAwithAE}
\citation{AEflow}
\citation{yin2023dino}
\citation{fathony2021multiplicative}
\citation{dupont2022}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}ERA5 datasets}{7}{subsubsection.4.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}network architectures and training process}{7}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}autoencoder}{7}{subsubsection.4.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}implicit neural representations}{7}{subsubsection.4.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}surrogate model for latent dynamics}{7}{subsubsection.4.2.3}}
\citation{Peyron2021LAwithAE}
\citation{bachlechner2021rezero}
\citation{Weller2012SkipLatLon}
\citation{kingma2014adam}
\citation{Peyron2021LAwithAE}
\citation{chen2018NeuralODE}
\citation{AEflow}
\citation{bachlechner2021rezero}
\citation{fathony2021multiplicative}
\citation{chen2018NeuralODE}
\citation{chen2021eventfn}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}training process}{8}{subsubsection.4.2.4}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Three different methods for comparison in the experiments.\relax }}{9}{table.caption.4}}
\newlabel{tab:comp_methods}{{3}{9}{Three different methods for comparison in the experiments.\relax }{table.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Distributions of the 5000 largest eigenvalues of the covariance matrix. The 400-th and the 1024-th eigenvalues are marked on the graph.\relax }}{9}{figure.caption.5}}
\newlabel{fig:eigvals}{{2}{9}{Distributions of the 5000 largest eigenvalues of the covariance matrix. The 400-th and the 1024-th eigenvalues are marked on the graph.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Performances}{9}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}exploring low-dimensional representations}{9}{subsubsection.4.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparison of different ROMs\relax }}{10}{figure.caption.6}}
\newlabel{fig:comp_eff}{{3}{10}{Comparison of different ROMs\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}embedding the physical dynamics}{10}{subsubsection.4.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}compatibility (not finished yet)}{10}{subsubsection.4.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparison of multi-step prediction error for different methods on the testing dataset\relax }}{11}{figure.caption.7}}
\newlabel{fig:comp-multi-step}{{4}{11}{Comparison of multi-step prediction error for different methods on the testing dataset\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion and future work}{11}{section.5}}
\@writefile{toc}{\contentsline {paragraph}{Non-linear embedding}{11}{section*.13}}
\@writefile{toc}{\contentsline {paragraph}{Continuous mapping}{11}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{Flexibility of inputs}{11}{section*.15}}
\@writefile{toc}{\contentsline {paragraph}{Scalability}{11}{section*.16}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces AEflow-ReZero with ETKF-Q, mod\_sigma=0.1, $n_\textrm  {obs}=1024$. The first 3 rows correspond to feature 0, while the last 3 rows correspond to feature 1. For each feature, the ground truth, the assimilated field as well as the corresponding differences are displayed sequentially (the frames are recorded every $40\Delta t$).\relax }}{12}{figure.caption.10}}
\newlabel{fig:aeflow-rezero-ass-plot}{{5}{12}{AEflow-ReZero with ETKF-Q, mod\_sigma=0.1, $n_\textrm {obs}=1024$. The first 3 rows correspond to feature 0, while the last 3 rows correspond to feature 1. For each feature, the ground truth, the assimilated field as well as the corresponding differences are displayed sequentially (the frames are recorded every $40\Delta t$).\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Our model with EnKF, mod\_sigma=0.1, $n_\textrm  {obs}=1024$. The first 3 rows and the last 3 rows correspond to the height and the vorticity, respectively. For each feature, the ground truth, the assimilated field as well as the corresponding differences are displayed sequentially.\relax }}{13}{figure.caption.11}}
\newlabel{fig:ours-ass-plot}{{6}{13}{Our model with EnKF, mod\_sigma=0.1, $n_\textrm {obs}=1024$. The first 3 rows and the last 3 rows correspond to the height and the vorticity, respectively. For each feature, the ground truth, the assimilated field as well as the corresponding differences are displayed sequentially.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Assimilation error (left: AEflow-ReZero with ETKF-Q, mod\_sigma=0.1, $n_\textrm  {obs}=1024$; right: INR-NeuralODE (ours) with SEnKF, mod\_sigma=0.05, $n_\textrm  {obs}=1024$)\relax }}{13}{figure.caption.12}}
\newlabel{fig:ass-err-comp}{{7}{13}{Assimilation error (left: AEflow-ReZero with ETKF-Q, mod\_sigma=0.1, $n_\textrm {obs}=1024$; right: INR-NeuralODE (ours) with SEnKF, mod\_sigma=0.05, $n_\textrm {obs}=1024$)\relax }{figure.caption.12}{}}
\citation{ERA5}
\bibstyle{plain}
\bibdata{LAINR}
\bibcite{Arcomano2020MLGAFM}{1}
\bibcite{Bocquet2016}{2}
\bibcite{bachlechner2021rezero}{3}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Comparison of performance metrics for different filters for AEflow-ReZero model.\relax }}{14}{table.caption.8}}
\newlabel{tab:aeflow_comparison}{{4}{14}{Comparison of performance metrics for different filters for AEflow-ReZero model.\relax }{table.caption.8}{}}
\bibcite{Bemana2020xfields}{4}
\bibcite{Pangu}{5}
\bibcite{SINDy}{6}
\bibcite{dedalus}{7}
\bibcite{FengWu}{8}
\bibcite{chen2023crom}{9}
\bibcite{chen2021eventfn}{10}
\bibcite{chen2018NeuralODE}{11}
\bibcite{chen2019learning}{12}
\bibcite{GeneralizedLA}{13}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Comparison of performance metrics for different filters for INR-NeuralODE (ours).\relax }}{15}{table.caption.9}}
\newlabel{tab:yin2023_comparison}{{5}{15}{Comparison of performance metrics for different filters for INR-NeuralODE (ours).\relax }{table.caption.9}{}}
\bibcite{GRU}{14}
\bibcite{Clare2021}{15}
\bibcite{dupont2022}{16}
\bibcite{dupont2022coinpp}{17}
\bibcite{fathony2021multiplicative}{18}
\bibcite{Fillion2020IEnKS}{19}
\bibcite{Galewsky-2004}{20}
\bibcite{AEflow}{21}
\bibcite{He2015ResNet}{22}
\bibcite{ERA5}{23}
\bibcite{LSTM}{24}
\bibcite{Jiang2020MeshfreeFlowNet}{25}
\bibcite{kingma2014adam}{26}
\bibcite{GraphCast}{27}
\bibcite{FNO}{28}
\bibcite{PDENet2.0}{29}
\bibcite{DeepONet}{30}
\bibcite{ReservoirComputing}{31}
\bibcite{Nicholas2015QGsetting}{32}
\bibcite{park2019deepsdf}{33}
\bibcite{FourCastNet}{34}
\bibcite{ROM-DA}{35}
\bibcite{LatentspaceDA-RNN}{36}
\bibcite{Peyron2021LAwithAE}{37}
\bibcite{PINNs}{38}
\bibcite{Rasp2020WeatherBench}{39}
\bibcite{Rasp2021}{40}
\bibcite{Scher2021Ensemble}{41}
\bibcite{Schultz2021CanDL}{42}
\bibcite{DeepGalerkin}{43}
\bibcite{sitzmann2019siren}{44}
\bibcite{AI-Feyman}{45}
\bibcite{DeepRitz}{46}
\bibcite{Weller2012SkipLatLon}{47}
\bibcite{Weyn2020}{48}
\bibcite{yin2023dino}{49}
\@writefile{toc}{\contentsline {section}{\numberline {A}Mathematical preliminaries}{18}{appendix.A}}
\newlabel{prop:pod_optimality}{{A.1}{18}{}{Prop.A.1}{}}
\citation{Galewsky-2004}
\citation{Nicholas2015QGsetting}
\citation{Galewsky-2004}
\citation{dedalus}
\@writefile{toc}{\contentsline {section}{\numberline {B}Detailed configurations for the experiments}{19}{appendix.B}}
\newlabel{sec:exp_configs}{{B}{19}{Detailed configurations for the experiments}{appendix.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}shallow-water equations}{19}{subsection.B.1}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Algorithms for associated Kalman filter methods}{19}{appendix.C}}
\newlabel{tab:kalman_filter_methods}{{C}{19}{Algorithms for associated Kalman filter methods}{appendix.C}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}EnKF}{19}{subsection.C.1}}
\citation{Bocquet2016}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.1.1}Forecast step}{20}{subsubsection.C.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.1.2}Analysis step}{20}{subsubsection.C.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}SEnKF}{20}{subsection.C.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.2.1}Forecast step}{20}{subsubsection.C.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.2.2}Analysis step}{20}{subsubsection.C.2.2}}
\citation{Bocquet2016}
\citation{Bocquet2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}DEnKF}{21}{subsection.C.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.3.1}Forecast step}{21}{subsubsection.C.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.3.2}Analysis step}{21}{subsubsection.C.3.2}}
\citation{Bocquet2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.4}EnSRKF}{22}{subsection.C.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.4.1}Forecast step}{22}{subsubsection.C.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.4.2}Analysis step}{22}{subsubsection.C.4.2}}
\citation{Peyron2021LAwithAE}
\citation{Fillion2020IEnKS}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.5}ETKF}{23}{subsection.C.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.5.1}Forecast step}{23}{subsubsection.C.5.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.5.2}Analysis step}{23}{subsubsection.C.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.6}ETKF-Q}{23}{subsection.C.6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.6.1}Initialization}{24}{subsubsection.C.6.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.6.2}Forecast step}{24}{subsubsection.C.6.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.6.3}Analysis step}{24}{subsubsection.C.6.3}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Implementation}{24}{appendix.D}}
